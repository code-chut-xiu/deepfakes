{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, ViTImageProcessor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels, transform):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[self.images[idx]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Google Vision Transformer\n",
    "PRETRAINED_MODEL = \"google/vit-base-patch16-224\"\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(PRETRAINED_MODEL)\n",
    "processor = ViTImageProcessor.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Au_txt_30028.jpg': 0, 'Au_txt_30026.jpg': 0, 'Au_txt_30027.jpg': 0, 'Au_txt_30025.jpg': 0, 'Au_txt_30024.jpg': 0, 'Tp_S_NRN_S_O_cha00077_cha00077_11017.jpg': 1, 'Tp_S_NRN_S_O_cha10187_cha10187_12308.jpg': 1, 'Tp_S_NRN_S_O_arc10129_arc10129_11895.jpg': 1, 'Tp_S_NRN_S_O_cha10126_cha10126_12153.jpg': 1, 'Tp_S_NRN_S_O_cha00035_cha00067_11734.jpg': 1}\n"
     ]
    }
   ],
   "source": [
    "# Prepare the Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "IMAGE_PATH = \"./private_samples/\"\n",
    "AUTHENTIC = IMAGE_PATH + \"authentic\"\n",
    "TAMPERED = IMAGE_PATH + \"tampered\"\n",
    "TEMP_IMAGE_FOLDER = \"./sample_images\"\n",
    "\n",
    "labels = {}\n",
    "if os.path.isdir(TEMP_IMAGE_FOLDER):\n",
    "    shutil.rmtree(TEMP_IMAGE_FOLDER)\n",
    "os.mkdir(TEMP_IMAGE_FOLDER)\n",
    "\n",
    "for f in os.listdir(AUTHENTIC):\n",
    "    labels[f] = 0\n",
    "    shutil.copy(AUTHENTIC + \"/\" + f, TEMP_IMAGE_FOLDER)\n",
    "\n",
    "for f in os.listdir(TAMPERED):\n",
    "    labels[f] = 1\n",
    "    shutil.copy(TAMPERED + \"/\" + f, TEMP_IMAGE_FOLDER)\n",
    "\n",
    "print(labels)\n",
    "    \n",
    "dataset = DeepfakeDataset(image_dir=TEMP_IMAGE_FOLDER, labels=labels, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.7841014862060547\n",
      "Epoch 2: Loss = 3.4363410472869873\n",
      "Epoch 3: Loss = 1.3409761190414429\n",
      "Epoch 4: Loss = 1.1505000591278076\n",
      "Epoch 5: Loss = 0.981009840965271\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for images, labels in data_loader:\n",
    "        inputs = processor(images, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
