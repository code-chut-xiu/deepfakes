{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributing images. Mixtures of fake and real go to test and train folders\n",
    "IMAGE_PATH = \"/Users/Hung.Le/Downloads/df-training-images/\"\n",
    "SOURCE = 'source/'\n",
    "REAL = 'real/'\n",
    "FAKE = 'fake/'\n",
    "AUTHENTIC = IMAGE_PATH + SOURCE + REAL\n",
    "TAMPERED = IMAGE_PATH + SOURCE + FAKE\n",
    "ELA = 'ela/'\n",
    "EDGE = 'edge/'\n",
    "LAPLACIAN = 'laplacian/'\n",
    "TRAIN_IMAGE_FOLDER = \"./train_images\"\n",
    "TEST_IMAGE_FOLDER = \"./test_images\"\n",
    "TRAIN_PERCENTAGE = 0.9\n",
    "TEST_PERCENTAGE = 1 - TRAIN_PERCENTAGE\n",
    "\n",
    "def create_labels() -> dict:\n",
    "    labels = {}\n",
    "    for f in os.listdir(AUTHENTIC):\n",
    "        labels[f] = 0\n",
    "    for f in os.listdir(TAMPERED):\n",
    "        labels[f] = 1\n",
    "    return labels\n",
    "\n",
    "def create_data_folder(target_image_dir: str, percent: float) -> dict:\n",
    "    labels = {}\n",
    "    if os.path.isdir(target_image_dir):\n",
    "        shutil.rmtree(target_image_dir) \n",
    "    os.mkdir(target_image_dir)\n",
    "    \n",
    "    authentic_list = os.listdir(AUTHENTIC)\n",
    "    print(len(authentic_list))\n",
    "    authentic_size = int(len(authentic_list) * percent)\n",
    "    selected_authentic_list = random.sample(authentic_list, k=authentic_size)\n",
    "    for f in selected_authentic_list:\n",
    "        labels[f] = 0\n",
    "        shutil.copy(AUTHENTIC + \"/\" + f, target_image_dir)\n",
    "    \n",
    "    tampered_list = os.listdir(TAMPERED)\n",
    "    print(len(tampered_list))\n",
    "    tampered_size = int(len(tampered_list) * percent)\n",
    "    selected_tampered_list = random.sample(tampered_list, k=tampered_size)\n",
    "    for f in selected_tampered_list:\n",
    "        labels[f] = 1\n",
    "        shutil.copy(TAMPERED + \"/\" + f, target_image_dir)\n",
    "\n",
    "    print(\"Number of images = \" + str(len(os.listdir(target_image_dir))))\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction functions\n",
    "def convert_to_ela_image(image_path, quality=90):\n",
    "    original = Image.open(image_path).convert(\"RGB\")\n",
    "    resaved_path = \"temp.jpg\"\n",
    "    original.save(resaved_path, \"JPEG\", quality=quality)\n",
    "    resaved = Image.open(resaved_path)\n",
    "    \n",
    "    ela_image = Image.fromarray(\n",
    "        np.abs(np.array(original, dtype=np.int16) - np.array(resaved, dtype=np.int16)).astype(np.uint8)\n",
    "    )\n",
    "    return ela_image\n",
    "\n",
    "def compute_edge_density(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    edges = cv2.Canny(img, 100, 200)\n",
    "    return Image.fromarray(edges)\n",
    "\n",
    "def compute_laplacian(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    laplacian = cv2.Laplacian(img, cv2.CV_64F)\n",
    "    laplacian = np.uint8(np.abs(laplacian))\n",
    "    return Image.fromarray(laplacian)\n",
    "\n",
    "def laplacian_variance(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "    return np.var(laplacian)\n",
    "\n",
    "def compute_noise_map(image_path):\n",
    "    # Read grayscale image\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Apply Gaussian blur to smooth the image\n",
    "    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    # Compute residual noise (high-frequency component)\n",
    "    noise_map = cv2.absdiff(img, blurred)\n",
    "    # Normalize to full range [0, 255]\n",
    "    norm_noise = cv2.normalize(noise_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    # Convert to PIL Image\n",
    "    return Image.fromarray(norm_noise.astype(np.uint8))\n",
    "\n",
    "def compute_frequency_map(image_path):\n",
    "    # Read grayscale image\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Convert to float32 and compute FFT\n",
    "    f = np.fft.fft2(img)\n",
    "    fshift = np.fft.fftshift(f)  # Shift zero freq to center\n",
    "    # Compute magnitude spectrum (log scale)\n",
    "    magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1e-5)  # avoid log(0)\n",
    "    # Normalize to [0, 255]\n",
    "    magnitude_spectrum = cv2.normalize(magnitude_spectrum, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    # Convert to PIL image\n",
    "    return Image.fromarray(magnitude_spectrum.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to set up dataset\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.image_files = list(labels.keys())  # Image filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        label = self.labels[img_name]\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Generate feature maps\n",
    "        ela_img = convert_to_ela_image(img_path).convert(\"L\")\n",
    "        edge_img = compute_edge_density(img_path).convert(\"L\")\n",
    "        lap_img = compute_laplacian(img_path).convert(\"L\")\n",
    "        noise_img = compute_noise_map(img_path).convert(\"L\")\n",
    "        fft_img = compute_frequency_map(img_path).convert(\"L\")\n",
    "\n",
    "        # Convert to tensors\n",
    "        ela_tensor = self.transform(ela_img)\n",
    "        edge_tensor = self.transform(edge_img)\n",
    "        lap_tensor = self.transform(lap_img)\n",
    "        noise_tensor = self.transform(noise_img)\n",
    "        fft_tensor = self.transform(fft_img)\n",
    "\n",
    "        # Stack into 5-channel input\n",
    "        input_tensor = torch.cat([ela_tensor, edge_tensor, lap_tensor, noise_tensor, fft_tensor], dim=0)\n",
    "        return input_tensor, torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and Modify the model\n",
    "def get_vgg16_model():\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    old_conv = model.features[0]\n",
    "\n",
    "    new_conv = nn.Conv2d(5, 64, kernel_size=3, stride=1, padding=1)\n",
    "    with torch.no_grad():\n",
    "        new_conv.weight[:, :3, :, :] = old_conv.weight\n",
    "        nn.init.kaiming_normal_(new_conv.weight[:, 3:, :, :])\n",
    "        new_conv.bias = old_conv.bias\n",
    "\n",
    "    model.features[0] = new_conv\n",
    "    model.classifier[6] = nn.Linear(4096, 2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=[\"Real\", \"Fake\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7437\n",
      "5123\n",
      "Number of images = 11303\n",
      "11303\n",
      "7437\n",
      "5123\n",
      "Number of images = 1255\n",
      "1255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hung.Le/workspace/Projects/Personal/ml/deepfakes/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/Hung.Le/workspace/Projects/Personal/ml/deepfakes/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.6448, Accuracy: 0.6271\n",
      "Epoch 2/10 - Loss: 0.5815, Accuracy: 0.6811\n",
      "Epoch 3/10 - Loss: 0.5027, Accuracy: 0.7436\n",
      "Epoch 4/10 - Loss: 0.4232, Accuracy: 0.7955\n",
      "Epoch 5/10 - Loss: 0.3767, Accuracy: 0.8208\n",
      "Epoch 6/10 - Loss: 0.3390, Accuracy: 0.8408\n",
      "Epoch 7/10 - Loss: 0.3142, Accuracy: 0.8517\n",
      "Epoch 8/10 - Loss: 0.2976, Accuracy: 0.8606\n",
      "Epoch 9/10 - Loss: 0.2783, Accuracy: 0.8692\n",
      "Epoch 10/10 - Loss: 0.2593, Accuracy: 0.8791\n",
      "Model saved!\n",
      "Confusion Matrix:\n",
      "[[625 118]\n",
      " [ 27 485]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.96      0.84      0.90       743\n",
      "        Fake       0.80      0.95      0.87       512\n",
      "\n",
      "    accuracy                           0.88      1255\n",
      "   macro avg       0.88      0.89      0.88      1255\n",
      "weighted avg       0.90      0.88      0.89      1255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_labels = create_data_folder(TRAIN_IMAGE_FOLDER, TRAIN_PERCENTAGE)\n",
    "print(len(train_labels))\n",
    "test_labels = create_data_folder(TEST_IMAGE_FOLDER, 1 - TRAIN_PERCENTAGE)\n",
    "print(len(test_labels))\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = DeepfakeDataset(TRAIN_IMAGE_FOLDER, train_labels, transform)\n",
    "test_dataset = DeepfakeDataset(TEST_IMAGE_FOLDER, test_labels, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "# Build and train\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = get_vgg16_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Tuning #1 - Changed learning rate from 1e-3 to 1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, device, epochs=10)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"vgg16_deepfake.pth\")\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Load and test\n",
    "model = get_vgg16_model()\n",
    "model.load_state_dict(torch.load(\"vgg16_deepfake.pth\", map_location=device))\n",
    "test_model(model, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
