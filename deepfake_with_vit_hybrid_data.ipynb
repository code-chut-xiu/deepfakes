{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, ViTImageProcessor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels, transform):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[self.images[idx]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Google Vision Transformer\n",
    "PRETRAINED_MODEL = \"google/vit-base-patch16-224\"\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(PRETRAINED_MODEL)\n",
    "processor = ViTImageProcessor.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8550\n",
      "949\n"
     ]
    }
   ],
   "source": [
    "# Prepare the Dataset\n",
    "IMAGE_PATH = \"/Users/Hung.Le/Downloads/df-training-images/\"\n",
    "AUTHENTIC = IMAGE_PATH + \"authentic\"\n",
    "TAMPERED = IMAGE_PATH + \"tampered\"\n",
    "TRAIN_IMAGE_FOLDER = \"./train_images\"\n",
    "TEST_IMAGE_FOLDER = \"./test_images\"\n",
    "TRAIN_PERCENTAGE = 0.9\n",
    "TEST_PERCENTAGE = 1 - TRAIN_PERCENTAGE\n",
    "\n",
    "def create_dataloader(target_image_dir: str, percent: float):\n",
    "    labels = {}\n",
    "    if os.path.isdir(target_image_dir):\n",
    "        shutil.rmtree(target_image_dir) \n",
    "    os.mkdir(target_image_dir)\n",
    "    \n",
    "    authentic_list = os.listdir(AUTHENTIC)\n",
    "    authentic_size = int(len(authentic_list) * percent)\n",
    "    selected_authentic_list = random.sample(authentic_list, k=authentic_size)\n",
    "    for f in selected_authentic_list:\n",
    "        labels[f] = 0\n",
    "        shutil.copy(AUTHENTIC + \"/\" + f, target_image_dir)\n",
    "    \n",
    "    tampered_list = os.listdir(TAMPERED)\n",
    "    tampered_size = int(len(tampered_list) * percent)\n",
    "    selected_tampered_list = random.sample(tampered_list, k=tampered_size)\n",
    "    for f in selected_tampered_list:\n",
    "        labels[f] = 1\n",
    "        shutil.copy(TAMPERED + \"/\" + f, target_image_dir)\n",
    "\n",
    "    print(len(os.listdir(target_image_dir)))\n",
    "    \n",
    "    dataset = DeepfakeDataset(image_dir=target_image_dir, labels=labels, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "data_loader = create_dataloader(TRAIN_IMAGE_FOLDER, TRAIN_PERCENTAGE)\n",
    "test_data_loader = create_dataloader(TEST_IMAGE_FOLDER, TEST_PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.004411107394844294\n",
      "Epoch 2: Loss = 0.00601558992639184\n",
      "Epoch 3: Loss = 0.018084106966853142\n",
      "Epoch 4: Loss = 0.019938277080655098\n",
      "Epoch 5: Loss = 0.33111074566841125\n",
      "Epoch 6: Loss = 0.10103277117013931\n",
      "Epoch 7: Loss = 0.013694819062948227\n",
      "Epoch 8: Loss = 0.0006407600594684482\n",
      "Epoch 9: Loss = 0.2215084284543991\n",
      "Epoch 10: Loss = 0.013709559105336666\n",
      "Epoch 11: Loss = 0.0034562915097922087\n",
      "Epoch 12: Loss = 0.0002726317325141281\n",
      "Epoch 13: Loss = 9.593784488970414e-05\n",
      "Epoch 14: Loss = 0.0012407434405758977\n",
      "Epoch 15: Loss = 0.005967862904071808\n",
      "Epoch 16: Loss = 0.008372739888727665\n",
      "Epoch 17: Loss = 0.001102649257518351\n",
      "Epoch 18: Loss = 0.0014308117097243667\n",
      "Epoch 19: Loss = 0.00031265956931747496\n",
      "Epoch 20: Loss = 6.1032696976326406e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the dragon\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in data_loader:\n",
    "        inputs = processor(images, return_tensors=\"pt\", do_rescale=False, do_normalize=False)\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 78.29%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_data_loader:\n",
    "        inputs = processor(images, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"vit_deepfake_detector_first_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
