{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, ViTImageProcessor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels, transform):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[self.images[idx]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Google Vision Transformer\n",
    "PRETRAINED_MODEL = \"google/vit-base-patch16-224\"\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(PRETRAINED_MODEL)\n",
    "processor = ViTImageProcessor.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Dataset\n",
    "IMAGE_PATH = \"/Users/Hung.Le/Downloads/df-training-images/\"\n",
    "AUTHENTIC = IMAGE_PATH + \"source/real\"\n",
    "TAMPERED = IMAGE_PATH + \"source/fake\"\n",
    "TRAIN_IMAGE_FOLDER = \"./train_images\"\n",
    "TEST_IMAGE_FOLDER = \"./test_images\"\n",
    "TRAIN_PERCENTAGE = 0.9\n",
    "TEST_PERCENTAGE = 1 - TRAIN_PERCENTAGE\n",
    "\n",
    "def create_dataloader(target_image_dir: str, percent: float):\n",
    "    labels = {}\n",
    "    if os.path.isdir(target_image_dir):\n",
    "        shutil.rmtree(target_image_dir) \n",
    "    os.mkdir(target_image_dir)\n",
    "    \n",
    "    authentic_list = os.listdir(AUTHENTIC)\n",
    "    authentic_size = int(len(authentic_list) * percent)\n",
    "    selected_authentic_list = random.sample(authentic_list, k=authentic_size)\n",
    "    for f in selected_authentic_list:\n",
    "        labels[f] = 0\n",
    "        shutil.copy(AUTHENTIC + \"/\" + f, target_image_dir)\n",
    "    \n",
    "    tampered_list = os.listdir(TAMPERED)\n",
    "    tampered_size = int(len(tampered_list) * percent)\n",
    "    selected_tampered_list = random.sample(tampered_list, k=tampered_size)\n",
    "    for f in selected_tampered_list:\n",
    "        labels[f] = 1\n",
    "        shutil.copy(TAMPERED + \"/\" + f, target_image_dir)\n",
    "\n",
    "    print(\"Number of images = \" + str(len(os.listdir(target_image_dir))))\n",
    "    \n",
    "    dataset = DeepfakeDataset(image_dir=target_image_dir, labels=labels, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images = 8550\n",
      "Epoch 1: Loss = 0.36950746178627014\n",
      "Epoch 2: Loss = 0.7061669230461121\n",
      "Epoch 3: Loss = 0.150094673037529\n",
      "Epoch 4: Loss = 0.5439589023590088\n",
      "Epoch 5: Loss = 0.2041434794664383\n",
      "Epoch 6: Loss = 0.0003474567783996463\n",
      "Epoch 7: Loss = 0.5543035864830017\n",
      "Epoch 8: Loss = 0.0010337498970329762\n",
      "Epoch 9: Loss = 0.163868710398674\n",
      "Epoch 10: Loss = 0.0037351239006966352\n",
      "Epoch 11: Loss = 0.0014524428406730294\n",
      "Epoch 12: Loss = 0.001491726259700954\n",
      "Epoch 13: Loss = 0.1237998977303505\n",
      "Epoch 14: Loss = 0.23559431731700897\n",
      "Epoch 15: Loss = 0.09915665537118912\n",
      "Epoch 16: Loss = 0.2214786559343338\n",
      "Epoch 17: Loss = 0.015697797760367393\n",
      "Epoch 18: Loss = 0.002015285659581423\n",
      "Epoch 19: Loss = 0.008106004446744919\n",
      "Epoch 20: Loss = 0.16620177030563354\n"
     ]
    }
   ],
   "source": [
    "# Train the dragon\n",
    "data_loader = create_dataloader(TRAIN_IMAGE_FOLDER, TRAIN_PERCENTAGE)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in data_loader:\n",
    "        inputs = processor(images, return_tensors=\"pt\", do_rescale=False, do_normalize=False)\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images = 949\n",
      "Validation Accuracy: 78.29%\n"
     ]
    }
   ],
   "source": [
    "# Test the dragon\n",
    "data_loader = create_dataloader(TEST_IMAGE_FOLDER, TEST_PERCENTAGE)\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in data_loader:\n",
    "        inputs = processor(images, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"vit_deepfake_detector_first_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
